\documentclass[11pt]{article}

\usepackage{amsmath,amsfonts,amssymb,graphicx,setspace,authblk}
\usepackage{titlesec,blkarray, bm} %mathrsfs -- Tom could not install this package...needed?
\usepackage{float,afterpage}
\usepackage[running,mathlines]{lineno}
\usepackage[vmargin=1in,hmargin=1in]{geometry}
\usepackage[authoryear,sort]{natbib}
\usepackage[dvipsnames]{xcolor}

\usepackage{enumitem}
\setlist{topsep=.125em,itemsep=-0.15em,leftmargin=0.75cm}
\setlength{\parindent}{0.35in}

%\usepackage[sc]{mathpazo} %Like Palatino with extensive math support

\usepackage{lineno}
\renewcommand{\refname}{Literature Cited}
\renewcommand{\floatpagefraction}{0.98}
\renewcommand{\topfraction}{0.99}
\renewcommand{\textfraction}{0.05}

\clubpenalty = 10000
\widowpenalty = 10000

\sloppy 

\usepackage{ifpdf}
\ifpdf
\DeclareGraphicsExtensions{.pdf,.png,.jpg}
\usepackage{epstopdf}
\else
\DeclareGraphicsExtensions{.eps}
\fi

\DeclareMathOperator{\Ex}{\mathbb{E}}
\DeclareMathOperator{\var}{\textit{Var}}
\DeclareMathOperator{\cov}{\textit{Cov}}

\usepackage{hyperref}

%%%%%%%%% Macros to simplify using our notation 

\newcommand{\s}[1]{{#1}^{\#}}
\newcommand{\f}[1]{{#1}^{\flat}}
\newcommand{\sr}[1]{{#1}^{*}}
\newcommand{\br}[1]{\langle {#1} \rangle} 
\newcommand{\bs}{\backslash} 
\def\alphat{\widetilde{\alpha}}
\newcommand{\half}{\frac{1}{2}}

% commands for commenting
\newcommand{\tom}[2]{{\color{red}{#1}}\footnote{\textit{\color{red}{#2}}}}
\newcommand{\steve}[2]{{\color{blue}{#1}}\footnote{\textit{\color{blue}{#2}}}}

% Define Box environment for numbered boxes. 
\newcounter{box}
\newcommand{\boxnumber}{\addtocounter{box}{1} \thebox \thinspace}

\floatstyle{boxed}
\newfloat{Box}{tbph}{box}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
%%% Just for commenting
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage[dvipsnames]{xcolor}
\newcommand{\comment}{\textcolor{blue}}
\newcommand{\new}{\textcolor{red}}

\newcommand{\be}{\begin{equation}}
\newcommand{\ee}{\end{equation}}

\newcommand{\red}{\textcolor{red}}

\title{My, how you've grown: a practical guide to modeling size transitions for Integral Projection Model (IPM) applications}

\author[a]{Tom E.X. Miller\thanks{Corresponding author. Department of BioSciences, Rice University,
Houston, TX 77005-1827. Email: tom.miller@rice.edu Phone: 713-348-4218}}
\author[b]{Stephen P. Ellner}
\affil[a]{Department of BioSciences, Rice University,Houston, TX } 
\affil[b]{Department of Ecology and Evolutionary Biology, Cornell University, Ithaca, New York} 

\renewcommand\Authands{ and }

\sloppy

\begin{document}

\renewcommand{\baselinestretch}{1.25} 
\maketitle

\bigskip 

\noindent \textbf{Authorship statement:} All authors discussed all aspects of the research and
contributed to developing methods, analyzing data, and writing and revising the paper.  

\bigskip 

\noindent{\textbf{Data accessibility statement}: No original data appear in this paper. 
Should the paper be accepted, all computer scripts supporting the results will be archived in an 
appropriate public repository such as Dryad or Figshare, with the DOI included 
at the end of the article.

\newpage
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\spacing{1.25} 
\section*{Abstract} 


\section{Introduction}

Structured demographic models -- matrix and integral projection models (MPMs and IPMs) -- are powerful tools for data-driven modeling of population dynamics and viability that are widely used in basic and applied settings. 
In contrast to long-standing MPMs for populations with discrete structure (life stage, age class, etc.), IPMs are a relatively recent development \citep{easterling2000size} introduced to readily accomodate populations structured by continuous variables, most commonly size. 
A related \tom{innovation}{This `innovation' is not unique to IPMs but it was rare in MPMs before IPMs.} of the IPM framework is its emphasis on regression-based modeling of size-dependent vital rates as sub-models of the larger projection model. 

The standard workflow allows ecologists to assemble an IPM from data using familiar regression tools to statistically describe growth, survival, reprduction, and other demographic transitions as functions of size. 
The relative ease and flexibility of the regression-based approach, potentially rich with covariates (like experimental treatment effects) and hierarchical variance structures (i.e., random effects), has facilitated a growing body of IPM literature that examines how biotic or abiotic factors affect population dynamics \citep[e.g.,][]{schultz2017native,ozgul2010coupled} and explores the consequences of demographic heterogeneity associated with spatio-temporal variation \citep[e.g.,][]{crone2016contrasting,compagnoni2016effect}. 
The vital rate regressions are the bridge between the individual-level data and the population-level model and predictions; it is important to get them right.

Compared to other vital rates, growth is special. 
The regression sub-models for survival and reproduction provide the expected values of these vital rates as functions of size (we will use ``size'' as the name
for whatever continuous variable defines the population structure, which could instead be immune competence, mother's weight, etc.).   
However, modeling growth for an IPM is about more than the expected value: the full probability distribution of future size, conditioned on previous size, must be defined. 
This distribution is used to populate a growth `kernel' $G(z',z)$ that gives the probability of transitioning from any current size $z$ at time $t$ to any future size $z'$ at time $t+1$, and thus incorporates an element of luck: some individuals grow more than average and others less.
As long as survival and reproductive rates are size-dependent then the distribution of size transitions around their expected values should strongly influence IPM predictions. 

The original template for modeling size transitions in an IPM framework was provided by Easterling et al. \citeyear{easterling2000size}, who used simple linear regression and thus assumed that growth residuals were normally distributed with constant variance. 
Problems with this approach were apparent from the beginning: in their landmark case study, Easterling et al. detected non-constant variance of residuals with respect to fitted values, which motivated a two-step approach that fit size-dependence in residuals following a preceding fit of mean growth - an approach that has since been widely adopted (cite). 
However, even after accounting for heteroscedasticity, growth data may still deviate from the assumption of normally distributed residuals. 
Size transitions are often skewed such that large decreases in size are more common than increases \citep{peterson2019improving,salguero2010keeping} or vice versa \citep{stubberud2019effects}.
Size transitions may also exhibit excess kurtosis (`fat tails'), where extreme growth or shrinkage is more common than predicted by the tails of the normal distribution. 

The observation that the normal distribution may poorly describe how real organisms change in size has been made before. 
Several studies have emphasized that the growth kernel need not rely on the normal distribution, and that fits of the data to alternative distributions should be explored \citep{easterling2000size,peterson2019improving,rees2014building,williams2012avoiding}. 
Yet, default use of Gaussian growth kernels (often with non-constant variance) remains the standard practice. 
An ISI Web of Knowledge search on the terms `integral projection model ecology' (DATE) returned \# IPM studies published in the past decade (2010--2020), \# of which assumed a Gaussian growth kernel\tom{}{Not sure if this is worth the trouble of doing, so I am just writing in placeholders for now. I think I know what I would find.}.
The general state-of-the-art in the literature appears stuck where it was 15 or so years ago, using the default model without pausing to examine critically whether or not it actually provides a good description of the data. 
We are guilty of this, ourselves. 
% Our goal here is to break that logjam, by building on Peterson et al. to provide a practical ``new default'' that researchers can follow to choose, fit, and implement better descriptions of individual growth for IPMs and any other size-structured population model.
% The default growth model traces back to Easterling et al. (2000). The analyses described there pre-date version 1.0 of R by several years, and they were quickly replaced by better methods such as the linear and generalized linear mixed models regression functions in R, notably the option to fit linear mixed models with nonconstant growth variance. 
% But as Peterson et al. recentlynoted, despite the growing list of other options and repeated calls in the literature for greater attention to growth modeling, the general state-of-the-art in the literature remains stuck where it was 15 or so years ago, using the default model without pausing to examine critically whether or not it actually provides a good description of the data. Our goal here is to break that logjam, by building on Peterson et al. to provide a practical ``new default'' that researchers can follow to choose, fit, and implement better descriptions of individual growth for IPMs and any other size-structured population model. 

The persistence of Gaussian growth modeling is understandable. 
There is a long tradition of statistical modeling built around the assumption of normally distributed residuals with constant variance (e.g., ANOVA).
Popular software pacakges such as lme4 \citep{bates2007lme4} and MCMCglmm \citep{hadfield2010mcmc} make it easy to fit growth models with potentially complex fixed- and random-effect structures, but the families of continuous response distributions they can accommodate are limited and default to Gaussian.
Abandoning these convenient and widely accesible tools for the sake of a more `exotic' growth kernel also means, it may seem, sacrificing the flexibility to rigorously model diverse and potentially complex sources of variation in growth, some of which may be the driving motivation for conducting the study in the first place.
The question, then, is how can ecologists break the apparent trade-off between realistically capturing the variance, skew, and kurtosis of size transition data on the one hand, and flexibly modeling the covariates and random effects that we know or suspect to be important on the other. 
In this afticle, we offer an answer. 

Our goal here is to develop a general `recipe' for growth modeling that moves the field beyond the standards set over 20 years ago.
Like any recipe, users may need to make substitutions or add flourishes to suit their situation. 
In our view, modeling growth in IPMs is \emph{modeling}, informed by statistical tools and tests but not dictated by their outcome. 
In particular, rather than proposing a process based on statistical model selection, we emphasize graphical diagnostics in developing and 
evaluating a growth model. Through a set of empirical case studies we demonstrate how a simple workflow, using tools 
that were nonexistent or not readily available when IPMs came into use, makes it straightforward and relatively 
easy to identify when the default choice (Gaussian with possibly nonconstant variance) is a poor fit to the data, 
and to then choose and fit a substantially better growth model that is no harder to use in practice. 

The remainder of this article is structured as follows:
\begin{itemize}
\item First, we outline the logic, key elements, and general workflow of our growth modeling approach.
\item Next, we apply this workflow to real data sets that come from our demographic studies of desert plants.
\item Then, we develop full IPMs for our desert plant case studies to evaluate how ``improved'' growth modeling affects IPM predictions\tom{}{I am still interested in possibly doing a simulation study to ask if there are particular types of life histories where getting the tails of size transitions right matters more or less. But punting that for now.}. 
\item Finally, we discuss things...
\end{itemize}
All of the code and data from this article are publicly available at \url{https://github.com/texmiller/IPM_size_transitions}.

\section{A general workflow for better growth modeling}

The modeling workflow that we suggest runs as follows:
\begin{enumerate}
\item Fit a ``pilot'' model with a Gaussian distribution of future size conditional on current size and any other covariates, 
having fitted non-constant variance. 
\item Use the fitted variance function to compute standardized residuals. If the Gaussian pilot model is valid, the set of standardized
residuals should be Gaussian with constant variance. 
\item Use statistical and graphical diagnostics, which we detail below, to identify if and how the standardized residuals deviate
from Gaussian: are there skew and kurtosis, and if so, how do those vary across the size distribution? If the standardized residuals
are as expected under the pilot model, skip to the last step. 
\item Otherwise, identify a distribution with nonzero skew and/or kurtosis that provides a good fit to the standardized residuals. Tools 
to automate this step are available in R. 
\item Refit the growth model using the chosen distribution, with shape (skew and kurtosis) varying according to the patterns 
seen in the graphical diagnostics. How this can be done depends on features of the model at hand, in particular whether or not it
includes random effects. 
\item Test the final model through graphical diagnostics comparing simulated growth increments with the data. A good model will generate
simulated data that look like the real data.   
\end{enumerate}
The assumption of non-constant variance in the pilot model means that it is not necessary to seek a data transformation that stabilizes
the growth variance. Transformation may have other benefits (e.g., log transformation avoids spurious negative sizes, or some other transformation
may be helpful in modeling survival or fecundity). However, it does entail some model selection to specify the mean-variance relationship of
future size. In contrast, we recommend that statistical model selection should be the basis for choosing the form of the final
regression model. Rather, its form should be chosen to match the observed properties of the scaled residuals, and at most slightly modified
based on final diagnostic tests.  


\section*{Acknowledgements} 
This research was supported by US NSF grants DEB-1933497 (SPE) and .... 

%\bibliographystyle{EcologyLetters}
\bibliographystyle{apalike}
\bibliography{BetterGrowthModeling}

% ######################## Appendices ##############################
\newpage 
\clearpage 
% \setcounter{page}{1}
\setcounter{equation}{0}
\setcounter{figure}{0}
\setcounter{section}{0}
\setcounter{table}{0}
\setcounter{Box}{0}
\renewcommand{\theequation}{S.\arabic{equation}}
\renewcommand{\thetable}{S-\arabic{table}}
\renewcommand{\thefigure}{S-\arabic{figure}}
\renewcommand{\theBox}{S-\arabic{Box}}
\renewcommand{\thesection}{S.\arabic{section}}

\centerline{\Large{\textbf{Appendices}}}

\section{Estimating mixed-effects models using shrinkage}

Ecologists often fit demographic and other statistical models that include random effects terms to
quantify variation among years, spatial locations, individuals, etc. Random effects
are a natural choice when interest centers on the magnitude of variation (e.g., how much does mortality vary among years?)  
rather than individual values (e.g., mortality in 2013). They also allow each estimate to 
``borrows strength'' from others, so that (for example) the estimate from a year with small sample size (and thus large 
sampling variability) is shifted towards the center of the overall distribution. 

Specialized software is often used to fit such models, such as the \textbf{nlme, lme4, mgcv} and \textbf{gamm4} libraries in R,  
but these only allow a small subset of the distribution families we want to consider for modeling growth increments (the \textbf{gamlss} 
package allows many distribution families, but in our experience, even when random effects are simple in structure 
the fitting algorithms often fail to converge or fail to find the global optimum). 

One way past this limitation is Bayesian estimation, using STAN with user-written (or borrowed) 
code for the chosen growth distribution (see section XX for an example). 
In this appendix we describe another option, introduced by \citet{gould-nichols-1998}: 
fitting in a fixed-effects framework followed by shrinkage of coefficient estimates. The 
material in this Appendix overlaps with Appendix S1 of \citet{metcalf-etal-2015}, but for completeness we make it self-contained. 

To illustrate shrinkage, we use a simple model based on our analysis of \emph{Pseudoroegneria spicata}. That model includes random effects
for between-year variation in the slope and intercept of future size (log of plant area) as a function of initial size. To keep the
example simple, we assume that initial size and year are the only covariates, and we assume that growth increments 
follow a skew-Normal distribution with nonconstant variance and constant skew parameter. 
As in our \emph{P. spicata} model, we use the GLM-style assumption that the location and scale parameters 
are both functions of the same linear predictor. Code is in the script \texttt{SimpleShrinkageExample-ML.R}. 

The analogous Gaussian model, with constant variance, could be fitted as follows using \texttt{lmer}:
\begin{verbatim}
lmer(new.size ~ init.size + (init.size|year), data=growthData, REML=TRUE); 
\end{verbatim}
where \texttt{growthData} is a data frame holding the data with \texttt{year} as an unordered factor. For our skew-Normal
model, we instead use maximum likelihood with all between-year variation included as fixed effects. The appropriate design
matrix is easily constructed using the \texttt{model.matrix} function: 
\begin{verbatim}
U = model.matrix(~year + init.size:year - 1, data=growthData)
\end{verbatim}
If there are $T$ years, the matrix \texttt{U} specified in this way has $2T$ columns corresponding to $n$ annual 
intercepts and $T$ annual slopes. 

Using this design matrix, we can readily write a log likelihood function for use with 
the \textbf{maxLik} package, with a log link function for the variance because it is necessarily positive: 
\begin{verbatim}
LogLik=function(pars,new.size,U){
	pars1 = pars[1:ncol(U)]; pars2=pars[-(1:ncol(U))];
	mu = U%*%pars1;  
	dSN1(new.size, mu=mu, sigma=exp(pars2[1]+pars2[2]*mu),nu=pars2[3],log=TRUE)
}
\end{verbatim} 

Parameters can then be estimated parameters with \texttt{maxLik}, starting from a random guess: 
\begin{verbatim}
start=c(runif(ncol(U)), rep(0,3))
out=maxLik(logLik=LogLik,start=start, new.size=simData$new.size,U=U,
			method="BHHH",control=list(iterlim=5000,printLevel=1),finalHessian=TRUE);
coefs = out$estimate; SEs = sqrt(diag(vcov(out)));			
\end{verbatim}  
In real life we would repeat the optimization several times with multiple starting values, to gain confidence that
the optimal parameter values had been found. 

Focus now on the year-specific intercept parameters $\hat{a}_t, t = 1,2,\cdots T$. Maximum likelihood also gives us a 
standard error estimate for each of these, $\hat{s}_t$ stored in the code above as the vectors \texttt{SEs}. Those
allow us to estimate the parameters of a random effects model for the intercepts, and thereby improve the year-specific
estimates. The model is as follows. We view the year-specific estimates $\hat{a}_t$ as consisting of unobserved true
values plus measurement error:
\be
\hat{a}_t= a_t + \varepsilon_t 
\ee
where the errors $\varepsilon_t$ have variances $s^2_t$. We then make the standard mixed-models assumptions that the $a_t$
are drawn independently from some fixed distribution with unknown variance $\sigma^2$, while the estimation errors satisfy 
\be
(i) \; \mathbb{E}(\varepsilon_t \vert a_t) = 0, \qquad (ii) \; Cov(\varepsilon_t, a_r) = 0 \mbox{ for all } t,r.  
\ee
In the present setting, these are optimistic assumptions. Assumption $(i)$ says that the year-specific estimates are unbiased.
This is not true for maximum likelihood, but if the assumptions of maximum likelihood are satisfied the bias is asympototically
negligible compared to the standard error (the bias scales as the inverse of sample size, the standard error as the square root
of the inverse of sample size). The second assumption is also unlikely to hold exactly, because the estimates for each year
are not based on separate data sets. Each year-specific estimate will be mainly based on data from that year, but whenever some 
model parameters are estimated from the full data set, each year-specific estimate is affected in part by data from other years. 
This result in cross-year correlations among estimates, and thus cross-year correlations in their deviations from true values. 

It follows from $(ii)$ that 
\be
Var(\hat{a_t}) = Cov(\hat{a_t}, \hat{a_t}) \sigma^2 + s_i^2
\ee















\end{document}